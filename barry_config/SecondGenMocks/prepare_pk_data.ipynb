{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82dae1ab-a0c9-4462-aaca-e3c75b3563cd",
   "metadata": {},
   "source": [
    "# Pickle Abacus SecondGen P(k) files for Barry\n",
    "This notebook includes code to ingest all the Abacus SecondGen files. See https://desi.lbl.gov/trac/wiki/keyprojects/y1kp3/clusteringproducts for details on the various types of mocks/data\n",
    "\n",
    "Barry expects that you will read in/specify and pickle:\n",
    "* The number of correlated datasets. **We set n_data == 1 below, but this would be 2 if we were providing i.e., NGC+SGC data vectors and wanted to consider different bias or polynomials for each cap**\n",
    "* Pre and post-recon data power spectrum with 5 multipoles (some multipoles can be set to zero if they are not required/measured). **Post-recon data currently set to None as it doesn't have the same redshift binning.**\n",
    "* N pre and post-recon mock power spectra with 5 multipoles (some can be set to zero if they are not required/measured). **Mocks currently set to None as we don't have these yet**\n",
    "* Pre and post-recon covariance matrices for the power spectra (some elements/blocks can be set to zero if they are not required/measured). **Pre-recon analytic used for both currently.**\n",
    "* A fiducial cosmology. **I've assumed the DESI fiducial cosmology**\n",
    "* A window function convolution/binning matrix (some elements/blocks can be set to the identify matrix if they are not required/measured), corresponding k-binning and integral constraint. Only needed for power spectra.\n",
    "* A compression matrix to convert the 3 even multipoles to 5 even+odd (can be given as a block identity matrix if you are not measuring odd multipoles). Only needed for power spectra.\n",
    "\n",
    "Correlation functions are similar but a little simpler (only 3 multipoles, and no window function stuff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fbb50-87dd-4a6a-bd26-c90c7d223fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages, set up the fiducial cosmology and save the DESI template\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from astropy.io import ascii\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import splrep, splev\n",
    "from cosmoprimo import PowerSpectrumBAOFilter\n",
    "from cosmoprimo.fiducial import DESI\n",
    "from pypower import BaseMatrix, CatalogFFTPower, CatalogFFTCorr, PowerSpectrumMultipoles, PowerSpectrumSmoothWindow, PowerSpectrumSmoothWindowMatrix, PowerSpectrumOddWideAngleMatrix, setup_logging\n",
    "from pycorr import TwoPointCorrelationFunction, project_to_multipoles\n",
    "\n",
    "cosmo = DESI()\n",
    "print(cosmo[\"Omega_b\"]*cosmo[\"h\"]**2, cosmo[\"Omega_cdm\"]*cosmo[\"h\"]**2, cosmo[\"Omega_m\"]*cosmo[\"h\"]**2 - cosmo[\"Omega_b\"]*cosmo[\"h\"]**2)\n",
    "print(cosmo[\"A_s\"], cosmo[\"n_s\"], cosmo[\"tau_reio\"])\n",
    "print(np.sum(cosmo[\"m_ncdm\"]))\n",
    "\n",
    "# Save the default DESI template to a file\n",
    "k_min = 1e-4\n",
    "k_max = 5\n",
    "k_num = 2000\n",
    "kl = np.logspace(np.log(k_min), np.log(k_max), k_num, base=np.e)\n",
    "pkz = cosmo.get_fourier().pk_interpolator()\n",
    "pk = pkz.to_1d(z=0)\n",
    "pkv = pk(kl)\n",
    "pknow = PowerSpectrumBAOFilter(pk, engine='wallish2018').smooth_pk_interpolator()\n",
    "pksmv = pknow(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd05d3-4e98-45f8-8dd2-67c65c06bc72",
   "metadata": {},
   "source": [
    "# Power spectrum routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa8518-6303-45c4-b15e-6dd8f31571d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful utility function to collate some Xi data\n",
    "def collect_pk_data(pre_files, post_files, pre_cov_files, post_cov_files, pre_files_name, post_files_name, pre_cov_name, post_cov_name, zs, reconsmooth, mocks, rpcut, imaging):\n",
    "\n",
    "    pre_data, post_data = None, None\n",
    "    \n",
    "    pre_mocks = get_pk(pre_files, pre_name, mocks, rpcut, imaging) if pre_files_name is not None else None\n",
    "    post_mocks = get_pk(post_files, post_name, mocks, rpcut, imaging) if post_files_name is not None else None\n",
    "    \n",
    "    pre_cov = get_pk_cov(pre_cov_files, pre_cov_name, rpcut, imaging) if pre_cov_files is not None else None\n",
    "    post_cov = get_pk_cov(post_cov_files, post_cov_name, rpcut, imaging) if post_cov_files is not None else None\n",
    "    \n",
    "    if pre_files is not None:\n",
    "        winmat, wam_reshape = getwin(pre_mocks[0][\"k\"].to_numpy(), post_files, post_name, rpcut, imaging)\n",
    "    else:\n",
    "        winmat, wam_reshape = getwin_dummy(pre_mocks[0][\"k\"].to_numpy())\n",
    "        \n",
    "    rp = f\" {imaging} rpcut2.5\" if rpcut else f\" {imaging}\" \n",
    "        \n",
    "    split = {\n",
    "        \"n_data\": 1,\n",
    "        \"pre-recon data\": pre_data,\n",
    "        \"pre-recon cov\": pre_cov,\n",
    "        \"post-recon data\": post_data,\n",
    "        \"post-recon cov\": post_cov,\n",
    "        \"pre-recon mocks\": pre_mocks,\n",
    "        \"post-recon mocks\": post_mocks,\n",
    "        \"cosmology\": {\n",
    "            \"om\": cosmo[\"Omega_m\"],\n",
    "            \"h0\": cosmo[\"h\"],\n",
    "            \"z\": (zs[1]+zs[0])/2.0,\n",
    "            \"ob\": cosmo[\"Omega_b\"],\n",
    "            \"ns\": cosmo[\"n_s\"],\n",
    "            \"mnu\": np.sum(cosmo[\"m_ncdm\"]),\n",
    "            \"reconsmoothscale\": reconsmooth,\n",
    "        },\n",
    "        \"name\": \"DESI SecondGen \" + f\"sm{reconsmooth} \" +  (\"_\").join(pre_files_name.split(\"_\")[1:]) + rp,\n",
    "        \"winfit\": winmat,\n",
    "        \"winpk\": None,  # We can set this to None; Barry will set it to zeroes given the length of the data vector.\n",
    "        \"m_mat\": wam_reshape,\n",
    "    }\n",
    "    \n",
    "    with open(f\"/global/cfs/cdirs/desi/users/chowlett/barry_inputs/DESI_SecondGen_sm{reconsmooth}_\" + (\"_\").join(pre_files_name.split(\"_\")[1:]).lower() + (\"_\").join(rp.split(\" \")) + \"_pk.pkl\", \"wb\") as f:\n",
    "        pickle.dump(split, f)\n",
    "        \n",
    "    return split\n",
    "\n",
    "# Power Spectrum\n",
    "def get_pk(loc, name, mocks, rpcut, imaging):\n",
    "    \n",
    "    rp = \"_rpcut2.5\" if rpcut else \"\" \n",
    "    \n",
    "    # Overwrite the <k> with the bin centres as we now use a binning matrix to correct to <P(k)>\n",
    "    ks = np.linspace(0.0, 0.4, 80, endpoint=False) + 0.0025\n",
    "    #ks = None\n",
    "    \n",
    "    pks = []\n",
    "    for mock in mocks:\n",
    "        if 'BGS_BRIGHT-21.5' in name and mock == 13:\n",
    "            continue\n",
    "        infile = loc + f\"/mock{mock}/\" + name + f\"_{imaging}_lin{rp}.npy\"\n",
    "\n",
    "        data = PowerSpectrumMultipoles.load(infile)\n",
    "        data.slice(slice(0,400,5))\n",
    "        df = pd.DataFrame(np.vstack(data(ell=[0,2,4], return_k=True)).T.real, columns=[\"k\", \"pk0\", \"pk2\", \"pk4\"])\n",
    "        df[\"pk1\"] = np.zeros(len(df[\"k\"]))\n",
    "        df[\"pk3\"] = np.zeros(len(df[\"k\"]))\n",
    "        df[\"nk\"] = data.nmodes\n",
    "        if ks is not None:\n",
    "            df[\"k\"] = ks\n",
    "        pks.append(df[[\"k\", \"pk0\", \"pk1\", \"pk2\", \"pk3\", \"pk4\"]])\n",
    "    \n",
    "    return pks\n",
    "    \n",
    "# Power Spectrum covariance matrix.\n",
    "def get_pk_cov(loc, name, rpcut, imaging):\n",
    "\n",
    "    rp = \"_rpcut2.5\" if rpcut else \"\" \n",
    "    infile = loc + \"cov_gaussian_pre_\" + name + f\"_{imaging}_lin{rp}.txt\"\n",
    "    \n",
    "    cov_input = pd.read_csv(infile, comment=\"#\", delim_whitespace=True, header=None).to_numpy()\n",
    "    nks = int(np.shape(cov_input)[0]/3)\n",
    "    nin = nks\n",
    "    cov = np.eye(5 * nks)\n",
    "    cov[:nks, :nks] = cov_input[:nks, :nks]\n",
    "    cov[:nks, 2 * nks : 3 * nks] = cov_input[:nks, nin : nin + nks]\n",
    "    cov[:nks, 4 * nks : 5 * nks] = cov_input[:nks, 2 * nin : 2 * nin + nks]\n",
    "    cov[2 * nks : 3 * nks, :nks] = cov_input[nin : nin + nks, :nks]\n",
    "    cov[2 * nks : 3 * nks, 2 * nks : 3 * nks] = cov_input[nin : nin + nks, nin : nin + nks]\n",
    "    cov[2 * nks : 3 * nks, 4 * nks : 5 * nks] = cov_input[nin : nin + nks, 2 * nin : 2 * nin + nks]\n",
    "    cov[4 * nks : 5 * nks, :nks] = cov_input[2 * nin : 2 * nin + nks, :nks]\n",
    "    cov[4 * nks : 5 * nks, 2 * nks : 3 * nks] = cov_input[2 * nin : 2 * nin + nks, nin : nin + nks]\n",
    "    cov[4 * nks : 5 * nks, 4 * nks : 5 * nks] = cov_input[2 * nin : 2 * nin + nks, 2 * nin : 2 * nin + nks]\n",
    "    \n",
    "    #plt.imshow(cov/np.sqrt(np.outer(np.diag(cov), np.diag(cov))))\n",
    "    #plt.show()\n",
    "    \n",
    "    # Check the covariance matrix is invertible\n",
    "    v = np.diag(cov @ np.linalg.inv(cov))\n",
    "    if not np.all(np.isclose(v, 1)):\n",
    "        print(\"ERROR, setting an inappropriate covariance matrix that is almost singular!!!!\")\n",
    "\n",
    "    return cov\n",
    "\n",
    "# Read's in window and wideangle matrices\n",
    "def getwin(ks, loc, name, rpcut, imaging):\n",
    "\n",
    "    rp = \"_rpcut2.5\" if rpcut else \"\" \n",
    "\n",
    "    #wam_data = BaseMatrix.load(\"/global/cfs/cdirs/desi/survey/catalogs/Y1/LSS/iron/LSScats/v0.6/blinded/pk/wmatrix_smooth_\" + (\"_\").join([name.split(\"/\")[-1].split(\"_\")[1]]+name.split(\"/\")[-1].split(\"_\")[4:]) + f\"_{imaging}_lin{rp}.npy\")\n",
    " \n",
    "    winname = (\"/\").join(name.split(\"/\")[:-1]) + \"/wmatrix_smooth_\" + (\"_\").join(name.split(\"/\")[-1].split(\"_\")[1:])\n",
    "    infile = loc + f\"/mock0/\" + winname + f\"_{imaging}_lin{rp}.npy\"\n",
    "    print(infile)\n",
    "    wam = BaseMatrix.load(infile)\n",
    "    \n",
    "    #plt.imshow(np.log10(np.fabs(wam.value)), aspect='auto')\n",
    "    #plt.show()\n",
    "    \n",
    "    wam = wam[:,:len(wam.xout[0])// 5 * 5]\n",
    "    #print(wam.xout[0], np.shape(wam.value))\n",
    "    old_wam = wam[:, :len(wam.xout[0]) // 5 * 5:5]\n",
    "    #print(np.shape(wam[:, :len(wam.xout[0]) // 5 * 5:5]))\n",
    "    wam.rebin_x(factorout=5)\n",
    "    #print(old_wam.value/wam.value)\n",
    "    kout = wam.xout[0] if ks is None else ks\n",
    "        \n",
    "    # This window function only has even multipoles as outputs and includes wide angle effects, so let's pad it with \n",
    "    # some zeros where the output odd multipoles would be so Barry is happy and then create a dummy wide angle matrix.\n",
    "    w_transform = np.zeros((5 * len(kout), 6 * len(wam.xin[0])))\n",
    "    wam_reshape = np.hsplit(wam.value, 3)\n",
    "    for j in range(3):\n",
    "        for i in range(3):\n",
    "            w_transform[2*j*len(kout): (2*j+1)*len(kout) , 2*i*len(wam.xin[0]) : (2*i+1)*len(wam.xin[0])] = wam_reshape[j][i*len(wam.xin[0]) : (i+1)*len(wam.xin[0])].T\n",
    "    \n",
    "    matrix = np.zeros((6 * len(wam.xin[0]), 3 * len(wam.xin[0])))\n",
    "    matrix[: len(wam.xin[0]), : len(wam.xin[0])] = np.diag(np.ones(len(wam.xin[0])))\n",
    "    matrix[2 * len(wam.xin[0]) : 3 * len(wam.xin[0]), len(wam.xin[0]) : 2 * len(wam.xin[0])] = np.diag(np.ones(len(wam.xin[0])))\n",
    "    matrix[4 * len(wam.xin[0]) : 5 * len(wam.xin[0]), 2 * len(wam.xin[0]) :] = np.diag(np.ones(len(wam.xin[0])))\n",
    "            \n",
    "    #plt.imshow(np.log10(np.fabs(w_transform)), aspect='auto')\n",
    "    #plt.show()\n",
    "\n",
    "    #plt.imshow((w_transform @ matrix).T, aspect='auto')\n",
    "    #plt.show()\n",
    "    \n",
    "    # The conversion matrix M from Beutler 2019. Used to compute the odd multipole models given the even multipoles. In the absence of wide angle effects, or if we don't care about\n",
    "    # the odd multipoles, we can set this to a block matrix with identity matrices in the appropriate places, as is done here.\n",
    "\n",
    "    res = {\"w_ks_input\": wam.xin[0], \"w_k0_scale\": np.zeros(len(wam.xin[0])), \"w_transform\": w_transform, \"w_ks_output\": kout}\n",
    "    winmat = {1: res}   # Step size is one, but we could modify this to contain other stepsizes too.\n",
    "    \n",
    "    # Wideangle matrix already included in window matrix, so pass None for wide-angle matrix so that Barry knows\n",
    "    return winmat, matrix\n",
    "    \n",
    "# Window function matrix. The window functions are stored in a dictionary of 'step sizes' i.e., how many bins get stuck together relative to the \n",
    "# pk measurements so that we can rebin the P(k) at run time if required. Each step size is a dictionary with:\n",
    "#    the input and output k binning (w_ks_input, w_ks_output), the window function matrix (w_transform) and integral constraint (w_k0_scale).\n",
    "# The window function assumes 6 input and 5 output multipoles. For cubic sims, we can set the integral constraint to zero and window matrix to a binning matrix, as is done here.\n",
    "def getwin_dummy(ks):\n",
    "    \n",
    "    dk = ks[1] - ks[0]\n",
    "    ks_input = np.logspace(-3.0, np.log10(0.5), 500)\n",
    "\n",
    "    binmat = np.zeros((len(ks), len(ks_input)))\n",
    "    for ii in range(len(ks_input)):\n",
    "\n",
    "        # Define basis vector\n",
    "        pkvec = np.zeros_like(ks_input)\n",
    "        pkvec[ii] = 1\n",
    "\n",
    "        # Define the spline:\n",
    "        pkvec_spline = splrep(ks_input, pkvec)\n",
    "\n",
    "        # Now compute binned basis vector:\n",
    "        tmp = np.zeros_like(ks)\n",
    "        for i, kk in enumerate(ks):\n",
    "            kl = kk - dk / 2\n",
    "            kr = kk + dk / 2\n",
    "            kin = np.linspace(kl, kr, 100)\n",
    "            tmp[i] = np.trapz(kin**2 * splev(kin, pkvec_spline, ext=3), x=kin) * 3 / (kr**3 - kl**3)\n",
    "\n",
    "        binmat[:, ii] = tmp\n",
    "\n",
    "    w_transform = np.zeros((5 * ks.size, 6 * ks_input.size))\n",
    "    for i in range(5):\n",
    "        w_transform[i*ks.size: (i+1)*ks.size , i*ks_input.size : (i+1)*ks_input.size] = binmat\n",
    "    \n",
    "    # The conversion matrix M from Beutler 2019. Used to compute the odd multipole models given the even multipoles. In the absence of wide angle effects, or if we don't care about\n",
    "    # the odd multipoles, we can set this to a block matrix with identity matrices in the appropriate places, as is done here.\n",
    "    matrix = np.zeros((6 * ks_input.size, 3 * ks_input.size))\n",
    "    matrix[: ks_input.size, : ks_input.size] = np.diag(np.ones(ks_input.size))\n",
    "    matrix[2 * ks_input.size : 3 * ks_input.size, ks_input.size : 2 * ks_input.size] = np.diag(np.ones(ks_input.size))\n",
    "    matrix[4 * ks_input.size : 5 * ks_input.size, 2 * ks_input.size :] = np.diag(np.ones(ks_input.size))\n",
    "    \n",
    "    res = {\"w_ks_input\": ks_input, \"w_k0_scale\": np.zeros(ks.size), \"w_transform\": w_transform, \"w_ks_output\": ks}\n",
    "    return {1: res}, matrix  # Step size is one    \n",
    "\n",
    "# Plot the correlation function, for sanity checking\n",
    "def plot_pk(split, pre=True, post=True):\n",
    "        \n",
    "    color = [\"r\", \"b\", \"g\"]\n",
    "    k = split[\"post-recon mocks\"][0][\"k\"]\n",
    "    nmocks = len(split[\"pre-recon mocks\"])\n",
    "    label = [r\"$P_{0}(k)$\", r\"$P_{2}(k)$\", r\"$P_{4}(k)$\"]\n",
    "        \n",
    "    if pre:\n",
    "        for m, pk in enumerate([\"pk0\", \"pk2\", \"pk4\"]):\n",
    "            yerr = k * np.sqrt(np.diag(split[\"pre-recon cov\"]))[m * len(k) : (m + 1) * len(k)]\n",
    "            plt.errorbar(\n",
    "                k,\n",
    "                k * np.mean([split[\"pre-recon mocks\"][i][pk] for i in range(nmocks)], axis=0),\n",
    "                yerr=yerr,\n",
    "                marker=\"o\",\n",
    "                ls=\"None\",\n",
    "                c=color[m],\n",
    "                label=label[m],\n",
    "            )\n",
    "            for i in range(nmocks):\n",
    "                plt.errorbar(k, k * split[\"pre-recon mocks\"][i][pk], marker=\"None\", ls=\"-\", c='k', alpha=1.0 / nmocks**(3.0/4.0))\n",
    "        plt.xlabel(r\"$k$\")\n",
    "        plt.ylabel(r\"$k\\,\\times pk(k)$\")\n",
    "        plt.title(split[\"name\"] + \" Prerecon\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "    if post:\n",
    "        for m, pk in enumerate([\"pk0\", \"pk2\", \"pk4\"]):\n",
    "            yerr = k * np.sqrt(np.diag(split[\"post-recon cov\"]))[m * len(k) : (m + 1) * len(k)]\n",
    "            plt.errorbar(\n",
    "                k,\n",
    "                k * np.mean([split[\"post-recon mocks\"][i][pk] for i in range(nmocks)], axis=0),\n",
    "                yerr=yerr,\n",
    "                marker=\"o\",\n",
    "                ls=\"None\",\n",
    "                c=color[m],\n",
    "                label=label[m],\n",
    "            )\n",
    "            for i in range(nmocks):\n",
    "                plt.errorbar(k, k * split[\"post-recon mocks\"][i][pk], marker=\"None\", ls=\"-\", c='k', alpha=1.0 / nmocks**(3.0/4.0))\n",
    "        plt.ylabel(r\"$k\\,\\times pk(k)$\")\n",
    "        plt.title(split[\"name\"] + \" Postrecon\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf4ba9-3f4f-4208-9aa1-f62187bc2d77",
   "metadata": {},
   "source": [
    "# Grab all the various datasets!!\n",
    "This will also plot the datasets, but this has been commented out in the GitHub version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860e2d9-2526-49c0-bb58-47452f4537bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The catalogue version\n",
    "version = 0.6\n",
    "ffa = \"ffa\"               # Flavour of fibre assignment. Can be \"ffa\" for fast fiber assign, or \"complete\"\n",
    "rpcut = False             # Whether or not to include the rpcut\n",
    "imaging = \"default_FKP\"   # What form of imaging systematics to use. Can be \"default_FKP\", \"default_FKP_addSN\", or \"default_FKP_addRF\"\n",
    "\n",
    "# This is a dictionary of all the combinations of dataset that we have and their redshift bins.\n",
    "tracers = {'BGS_BRIGHT-21.5': [[0.1,0.4]],\n",
    "           'LRG': [[0.4, 0.6], [0.6, 0.8], [0.8, 1.1]], \n",
    "           'ELG_LOP': [[0.8, 1.1], [1.1, 1.6]],\n",
    "           'QSO': [[0.8, 2.1]]}\n",
    "\n",
    "# How many complete mocks are available for each tracer? \n",
    "# While the mocks are still being processed, this allows us to skip over the missing entries\n",
    "nmocks = {'BGS_BRIGHT-21.5': [0,25], 'LRG': [0,25], 'ELG_LOP': [0,25], 'QSO': [0,25]}\n",
    "\n",
    "# This dictionary specifies the particulars of how reconstruction was run on each tracer. First entry is smoothing scale, second is type of recon. \n",
    "# QSO has no recon, so set to None so it can be skipped over later.\n",
    "recon = {'BGS_BRIGHT-21.5': [15, \"IFTrecsym\"],\n",
    "         'LRG': [10, \"IFTrecsym\"], \n",
    "         'ELG_LOP': [10, \"IFTrecsym\"],\n",
    "         'QSO': [30, \"IFTrecsym\"]}\n",
    "\n",
    "# The different sky areas\n",
    "caps = [\"NGC\", \"SGC\", \"GCcomb\"]\n",
    "\n",
    "basepath = f\"/global/cfs/cdirs/desi/survey/catalogs/Y1/mocks/SecondGenMocks/\"\n",
    "pre_cov_files = f\"/global/cfs/cdirs/desi/survey/catalogs/Y1/LSS/iron/LSScats/v{version}/blinded/pk/covariances/\"\n",
    "post_cov_files = f\"/global/cfs/cdirs/desi/survey/catalogs/Y1/LSS/iron/LSScats/v{version}/blinded/pk/covariances/\"    # No post recon covariances yet?\n",
    "\n",
    "# Now loop over each tracer, redshift bin and cap and gather the files. First pre-recon\n",
    "for t in tracers:\n",
    "    for i, zs in enumerate(tracers[t]):\n",
    "        for cap in caps:\n",
    "                    \n",
    "            pre_files = basepath + \"/AbacusSummitBGS/SecondGenMocks/AbacusSummit/\" if t == \"BGS_BRIGHT-21.5\" else basepath + \"/AbacusSummit/\"\n",
    "            post_files = basepath + \"/AbacusSummitBGS/SecondGenMocks/AbacusSummit/\" if t == \"BGS_BRIGHT-21.5\" else basepath + \"/AbacusSummit/\"\n",
    "            \n",
    "            pre_name =  f\"/pk/pkpoles_{t}_{ffa}_{cap}_{zs[0]}_{zs[1]}\"\n",
    "            if t == 'BGS_BRIGHT-21.5':\n",
    "                post_name = f\"/recon_sm{recon[t][0]}/pk/pkpoles_{t}_{ffa}_{recon[t][1]}_{cap}_{zs[0]}_{zs[1]}\" if recon[t][1] is not None else None\n",
    "            else:\n",
    "                post_name = f\"/recon_sm{recon[t][0]}/pk/recon_sm{recon[t][0]}/pk/pkpoles_{t}_{ffa}_{recon[t][1]}_{cap}_{zs[0]}_{zs[1]}\" if recon[t][1] is not None else None\n",
    "            tcov = \"ELG_LOPnotqso\" if \"ELG\" in t else t\n",
    "            pre_cov_name = f\"{tcov}_{cap}_{zs[0]}_{zs[1]}\"\n",
    "            #post_cov_name = f\"{tcov}_{recon[t][1]}_sm{recon[t][0]}_{cap}_{zs[0]}_{zs[1]}\" if recon[t][1] is not None else None\n",
    "            post_cov_name = pre_cov_name if recon[t][1] is not None else None\n",
    "\n",
    "            print(pre_name, post_name, pre_cov_name, post_cov_name)\n",
    "            data = collect_pk_data(pre_files, post_files, pre_cov_files, post_cov_files, pre_name, post_name, pre_cov_name, post_cov_name, zs, recon[t][0], range(nmocks[t][0], nmocks[t][1]), rpcut, imaging)\n",
    "            plot_pk(data, post=False if post_name is None else True) # Plot the data to check things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ab9ef-b812-4f37-85c0-d19ddaa60e87",
   "metadata": {},
   "source": [
    "# Test we can load the datasets in Barry\n",
    "This loads in the pickle file using the Barry routines, and then plots the data and a default model. Because I've set marg=\"full\", it will automatically find the best-fit polynomial terms too when plotting, but not the BAO parameters. If this works, a full MCMC BAO fit should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f8111-7241-4617-8c18-81c5cff12091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../../Barry/\")     # Change this so that it points to where you have Barry installed\n",
    "from barry.models import PowerBeutler2017, CorrBeutler2017\n",
    "from barry.datasets.dataset_power_spectrum import PowerSpectrum_DESI_KP4\n",
    "from barry.datasets.dataset_correlation_function import CorrelationFunction_DESI_KP4\n",
    "\n",
    "for t in tracers:\n",
    "    for i, zs in enumerate(tracers[t]):\n",
    "        for cap in caps:\n",
    "            rec = [None] if t == \"QSO\" else [None, \"sym\"]\n",
    "            for r in rec:\n",
    "                \n",
    "                rp = f\"{imaging}_rpcut2.5\" if rpcut else f\"{imaging}\"\n",
    "                name = f\"DESI_SecondGen_sm{recon[t][0]}_{t.lower()}_{ffa}_{cap.lower()}_{zs[0]}_{zs[1]}_{rp}_pk.pkl\"\n",
    "                dataset = PowerSpectrum_DESI_KP4(\n",
    "                    recon=r,\n",
    "                    fit_poles=[0, 2],\n",
    "                    min_k=0.02,\n",
    "                    max_k=0.30,\n",
    "                    realisation=None,\n",
    "                    reduce_cov_factor=len(range(nmocks[t][0], nmocks[t][1])),\n",
    "                    datafile=name,\n",
    "                    data_location=\"./\",\n",
    "                )\n",
    "\n",
    "                model = PowerBeutler2017(\n",
    "                    recon=dataset.recon,\n",
    "                    isotropic=dataset.isotropic,\n",
    "                    marg=\"full\",\n",
    "                    poly_poles=dataset.fit_poles,\n",
    "                )\n",
    "\n",
    "                model.sanity_check(dataset)\n",
    "                #model.set_data(dataset.get_data())\n",
    "                #model.plot(model.get_param_dict(model.get_defaults()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d41b5-48fd-4a87-9ddb-178fbed5eae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi-main",
   "language": "python",
   "name": "cosmodesi-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
