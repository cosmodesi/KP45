{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82dae1ab-a0c9-4462-aaca-e3c75b3563cd",
   "metadata": {},
   "source": [
    "# Pickle Y1 Xi files for Barry\n",
    "This notebook includes code to ingest all the Y1 blinded data files. Barry has a strict set of data inputs, and everything gets pickled up into this format. This means the underlying clustering measurements etc., can be in any format.\n",
    "\n",
    "Barry expects that you will read in/specify and pickle:\n",
    "* The number of correlated datasets. **We set n_data == 1 below, but this would be 2 if we were providing i.e., NGC+SGC data vectors and wanted to consider different bias or polynomials for each cap**\n",
    "* Pre and post-recon data power spectrum with 5 multipoles (some multipoles can be set to zero if they are not required/measured). **Post-recon data currently set to None as it doesn't have the same redshift binning.**\n",
    "* N pre and post-recon mock power spectra with 5 multipoles (some can be set to zero if they are not required/measured). **Mocks currently set to None as we don't have these yet**\n",
    "* Pre and post-recon covariance matrices for the power spectra (some elements/blocks can be set to zero if they are not required/measured). **Pre-recon analytic used for both currently.**\n",
    "* A fiducial cosmology. **I've assumed the DESI fiducial cosmology**\n",
    "* A window function convolution/binning matrix (some elements/blocks can be set to the identify matrix if they are not required/measured), corresponding k-binning and integral constraint. Only needed for power spectra.\n",
    "* A compression matrix to convert the 3 even multipoles to 5 even+odd (can be given as a block identity matrix if you are not measuring odd multipoles). Only needed for power spectra.\n",
    "\n",
    "Correlation functions are similar but a little simpler (only 3 multipoles, and no window function stuff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fbb50-87dd-4a6a-bd26-c90c7d223fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages, set up the fiducial cosmology and save the DESI template\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from astropy.io import ascii\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import splrep, splev\n",
    "from cosmoprimo import PowerSpectrumBAOFilter\n",
    "from cosmoprimo.fiducial import DESI\n",
    "from pypower import BaseMatrix, CatalogFFTPower, CatalogFFTCorr, PowerSpectrumMultipoles, PowerSpectrumSmoothWindow, PowerSpectrumSmoothWindowMatrix, PowerSpectrumOddWideAngleMatrix, setup_logging\n",
    "from pycorr import TwoPointCorrelationFunction, project_to_multipoles\n",
    "\n",
    "cosmo = DESI()\n",
    "print(cosmo[\"Omega_b\"]*cosmo[\"h\"]**2, cosmo[\"Omega_cdm\"]*cosmo[\"h\"]**2, cosmo[\"Omega_m\"]*cosmo[\"h\"]**2 - cosmo[\"Omega_b\"]*cosmo[\"h\"]**2)\n",
    "print(cosmo[\"A_s\"], cosmo[\"n_s\"], cosmo[\"tau_reio\"])\n",
    "print(np.sum(cosmo[\"m_ncdm\"]))\n",
    "\n",
    "# Save the default DESI template to a file\n",
    "k_min = 1e-4\n",
    "k_max = 5\n",
    "k_num = 2000\n",
    "kl = np.logspace(np.log(k_min), np.log(k_max), k_num, base=np.e)\n",
    "pkz = cosmo.get_fourier().pk_interpolator()\n",
    "pk = pkz.to_1d(z=0)\n",
    "pkv = pk(kl)\n",
    "pknow = PowerSpectrumBAOFilter(pk, engine='wallish2018').smooth_pk_interpolator()\n",
    "pksmv = pknow(kl)\n",
    "#np.savetxt(\"./DESI_Pk_template.dat\", np.c_[kl, pksmv, pkv/pksmv - 1.0],  fmt=\"%g %g %g\", header=\"k     pk_smooth     pk_ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd05d3-4e98-45f8-8dd2-67c65c06bc72",
   "metadata": {},
   "source": [
    "# Correlation function routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa8518-6303-45c4-b15e-6dd8f31571d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful utility function to collate some Xi data\n",
    "def collect_xi_data(pre_files, post_files, pre_mock_files, post_mock_files, pre_cov_files, post_cov_files, pre_files_name, post_files_name, pre_cov_name, post_cov_name, zs, version, reconsmooth, rpcut, imaging):\n",
    "\n",
    "    pre_data = get_xi2(pre_files, pre_files_name, rpcut, imaging) if pre_files is not None else None\n",
    "    post_data = get_xi2(post_files, post_files_name, rpcut, imaging) if post_files is not None else None\n",
    "    \n",
    "    pre_mocks = get_xi2_mock(pre_mock_files, pre_files_name, rpcut, imaging) if pre_mock_files is not None else None\n",
    "    post_mocks = get_xi2_mock(post_mock_files, post_files_name, rpcut, imaging) if post_mock_files is not None else None\n",
    "    #pre_mocks, post_mocks = None, None\n",
    "    \n",
    "    pre_cov = get_xi_cov(pre_cov_files, pre_cov_name, rpcut, imaging) if pre_cov_name is not None else None\n",
    "    post_cov = get_xi_cov(post_cov_files, post_cov_name, rpcut, imaging) if post_cov_name is not None else None\n",
    "    \n",
    "    rp = f\" {imaging} rpcut2.5\" if rpcut else f\" {imaging}\" \n",
    "    \n",
    "    split = {\n",
    "        \"n_data\": 1,\n",
    "        \"pre-recon data\": pre_data,\n",
    "        \"pre-recon cov\": pre_cov,\n",
    "        \"post-recon data\": post_data,\n",
    "        \"post-recon cov\": post_cov,\n",
    "        \"pre-recon mocks\": pre_mocks,\n",
    "        \"post-recon mocks\": post_mocks,\n",
    "        \"cosmology\": {\n",
    "            \"om\": cosmo[\"Omega_m\"],\n",
    "            \"h0\": cosmo[\"h\"],\n",
    "            \"z\": (zs[1]+zs[0])/2.0,\n",
    "            \"ob\": cosmo[\"Omega_b\"],\n",
    "            \"ns\": cosmo[\"n_s\"],\n",
    "            \"mnu\": np.sum(cosmo[\"m_ncdm\"]),\n",
    "            \"reconsmoothscale\": reconsmooth,\n",
    "        },\n",
    "        \"name\": \"DESI Y1 BLIND \" + f\"v{version} sm{reconsmooth} \" + pre_files_name + rp\n",
    "    }\n",
    "    \n",
    "    with open(f\"/global/cfs/cdirs/desi/users/chowlett/barry_inputs/DESI_Y1_BLIND_v{version}_sm{reconsmooth}_\" + pre_files_name.lower() + (\"_\").join(rp.split(\" \")) + \"_xi.pkl\", \"wb\") as f:\n",
    "        pickle.dump(split, f)\n",
    "        \n",
    "    return split\n",
    "\n",
    "# Correlation function\n",
    "def get_xi(loc, name, rpcut, imaging):\n",
    "    \n",
    "    rp = \"_rpcut2.5\" if rpcut else \"\" \n",
    "    nran = \"nran5\" if \"MGrec\" in name else \"nran4\"\n",
    "    infile = loc + \"xipoles_\" + name + f\"_{imaging}_lin4_njack0_{nran}_split20{rp}.txt\"\n",
    "    \n",
    "    xi = pd.read_csv(infile, comment=\"#\", skiprows=0, delim_whitespace=True, header=None, names=[\"s\", \"savg\",\"xi0\",\"xi2\",\"xi4\"])\n",
    "    xi = xi.drop(xi[xi[\"s\"] < 20.0].index)\n",
    "    \n",
    "    return [xi[[\"s\",\"xi0\",\"xi2\",\"xi4\"]]]\n",
    "\n",
    "# Correlation function\n",
    "def get_xi2(loc, name, rpcut, imaging):\n",
    "    \n",
    "    infile = loc + \"allcounts_\" + name + \".npy\"\n",
    "    \n",
    "    result = TwoPointCorrelationFunction.load(infile)\n",
    "    factor = 4\n",
    "    rebinned = result[:(result.shape[0] // factor) * factor:factor]\n",
    "    sep, xi = rebinned(ells=(0, 2, 4), return_sep=True, return_std=False)\n",
    "\n",
    "    xis = pd.DataFrame({'s': sep, 'xi0': xi[0], 'xi2': xi[1], 'xi4': xi[2]})\n",
    "    xis = xis.drop(xis[xis[\"s\"] < 20.0].index)\n",
    "    \n",
    "    return [xis[[\"s\",\"xi0\",\"xi2\",\"xi4\"]]]\n",
    "\n",
    "# Correlation function\n",
    "def get_xi2_mock(loc, name, rpcut, imaging):\n",
    "    \n",
    "    nmocks = 1000\n",
    "    \n",
    "    firstloc = (\"/\").join(loc.split(\"/\")[:-4]) if \"recon_recsym\" in loc else (\"/\").join(loc.split(\"/\")[:-3])\n",
    "    secondloc = (\"/\").join(loc.split(\"/\")[-4:]) if \"recon_recsym\" in loc else (\"/\").join(loc.split(\"/\")[-3:])\n",
    "    \n",
    "    xis = []\n",
    "    for i in range(1,nmocks):\n",
    "        infile = firstloc + f\"/mock{i}/\" + secondloc + \"allcounts_\" + name.replace(\"ELG_LOPnotqso\",\"ELG_LOP\") + \".npy\"\n",
    "        try:\n",
    "            result = TwoPointCorrelationFunction.load(infile)\n",
    "            factor = 4\n",
    "            rebinned = result[:(result.shape[0] // factor) * factor:factor]\n",
    "            sep, xi = rebinned(ells=(0, 2, 4), return_sep=True, return_std=False)\n",
    "\n",
    "            xidf = pd.DataFrame({'s': sep, 'xi0': xi[0], 'xi2': xi[1], 'xi4': xi[2]})\n",
    "            xidf = xidf.drop(xidf[xidf[\"s\"] < 20.0].index)\n",
    "            xis.append(xidf[[\"s\",\"xi0\",\"xi2\",\"xi4\"]])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return xis\n",
    "\n",
    "# Correlation function covariance matrix.\n",
    "def get_xi_cov(loc, name, rpcut, imaging):\n",
    "\n",
    "    s = \"rescaled\"\n",
    "    infile = loc + \"xi024_\" + name.replace(\"sm30\",\"sm20\") + f\"_{imaging}_lin4_s20-200_cov_RascalC_{s}.txt\"    # No recon_sm30 cov for QSO yet\n",
    "    \n",
    "    cov = pd.read_csv(infile, comment=\"#\", delim_whitespace=True, header=None).to_numpy()\n",
    "    \n",
    "    #plt.imshow(cov/np.sqrt(np.outer(np.diag(cov), np.diag(cov))))\n",
    "    #plt.show()\n",
    "    \n",
    "    # Check the covariance matrix is invertible\n",
    "    v = np.diag(cov @ np.linalg.inv(cov))\n",
    "    if not np.all(np.isclose(v, 1)):\n",
    "        print(\"ERROR, setting an inappropriate covariance matrix that is almost singular!!!!\")\n",
    "\n",
    "    return cov\n",
    "    \n",
    "# Plot the correlation function, for sanity checking\n",
    "def plot_xi(split, pre=True, post=True):\n",
    "\n",
    "    color = [\"r\", \"b\", \"g\"]\n",
    "    ss = split[\"pre-recon data\"][0][\"s\"]\n",
    "    nmocks = len(split[\"pre-recon mocks\"]) if split[\"pre-recon mocks\"] is not None else 0\n",
    "    label = [r\"$\\xi_{0}(k)$\", r\"$\\xi_{2}(k)$\", r\"$\\xi_{4}(k)$\"]\n",
    "    \n",
    "    if pre:\n",
    "        for m, xi in enumerate([\"xi0\", \"xi2\", \"xi4\"]):\n",
    "            yerr = ss ** 2 * np.sqrt(np.diag(split[\"pre-recon cov\"]))[m * len(ss) : (m + 1) * len(ss)]\n",
    "            plt.errorbar(\n",
    "                ss,\n",
    "                ss ** 2 * split[\"pre-recon data\"][0][xi],\n",
    "                yerr=yerr,\n",
    "                marker=\"o\",\n",
    "                ls=\"None\",\n",
    "                c=color[m],\n",
    "                label=label[m],\n",
    "            )\n",
    "            for i in range(nmocks):\n",
    "                plt.errorbar(ss, ss ** 2 * split[\"pre-recon mocks\"][i][xi], marker=\"None\", ls=\"-\", c='k', alpha=1.0 / nmocks**(3.0/4.0))\n",
    "        plt.xlabel(r\"$s$\")\n",
    "        plt.ylabel(r\"$s^{2}\\,\\xi(s)$\")\n",
    "        plt.title(split[\"name\"] + \" Prerecon\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "    if post:\n",
    "        for m, xi in enumerate([\"xi0\", \"xi2\", \"xi4\"]):\n",
    "            yerr = ss ** 2 * np.sqrt(np.diag(split[\"post-recon cov\"]))[m * len(ss) : (m + 1) * len(ss)]\n",
    "            plt.errorbar(\n",
    "                ss,\n",
    "                ss ** 2 * split[\"post-recon data\"][0][xi],\n",
    "                yerr=yerr,\n",
    "                marker=\"o\",\n",
    "                ls=\"None\",\n",
    "                c=color[m],\n",
    "                label=label[m],\n",
    "            )\n",
    "            for i in range(nmocks):\n",
    "                plt.errorbar(ss, ss ** 2 * split[\"post-recon mocks\"][i][xi], marker=\"None\", ls=\"-\", c='k', alpha=1.0 / nmocks**(3.0/4.0))\n",
    "        plt.xlabel(r\"$s$\")\n",
    "        plt.ylabel(r\"$s^{2}\\,\\xi(s)$\")\n",
    "        plt.title(split[\"name\"] + \" Postrecon\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf4ba9-3f4f-4208-9aa1-f62187bc2d77",
   "metadata": {},
   "source": [
    "# Grab all the various datasets!!\n",
    "This will also plot the datasets, but this has been commented out in the GitHub version for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2547572-a03b-49f0-a5c7-19d6d7f869c2",
   "metadata": {},
   "source": [
    "## Version 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6725131-a004-42b9-b452-9b8de8b40548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The catalogue version\n",
    "version = 0.6\n",
    "rpcut = False             # Whether or not to include the rpcut\n",
    "imaging = \"default_FKP\"   # What form of imaging systematics to use. Can be \"default_FKP\", \"default_FKP_addSN\", or \"default_FKP_addRF\"\n",
    "\n",
    "# This is a dictionary of all the combinations of dataset that we have and their redshift bins.\n",
    "tracers = {'BGS_BRIGHT-21.5': [[0.1, 0.4]], \n",
    "           'LRG': [[0.4, 0.6], [0.6, 0.8], [0.8, 1.1]], \n",
    "           'ELG_LOPnotqso': [[0.8, 1.1], [1.1, 1.6]],\n",
    "           'QSO': [[0.8, 2.1]]}\n",
    "\n",
    "# This dictionary specifies the particulars of how reconstruction was run on each tracer. First entry is smoothing scale, second is type of recon.\n",
    "recon = {'BGS_BRIGHT-21.5': [15, \"IFTrecsym\"], \n",
    "         'LRG': [10, \"IFTrecsym\"], \n",
    "         'ELG_LOPnotqso': [10, \"IFTrecsym\"],\n",
    "         'QSO': [20, \"IFTrecsym\"]}\n",
    "\n",
    "# The different sky areas\n",
    "caps = [\"NGC\", \"SGC\", \"GCcomb\"]\n",
    "\n",
    "basepath = f\"/global/cfs/cdirs/desi/survey/catalogs/Y1/LSS/iron/LSScats/v{version}/blinded/\"\n",
    "pre_cov_files = f\"/global/cfs/cdirs/desi/users/mrash/RascalC/Y1/blinded/v{version}/\"       \n",
    "post_cov_files = f\"/global/cfs/cdirs/desi/users/mrash/RascalC/Y1/blinded/v{version}/\"\n",
    "\n",
    "# Now loop over each tracer, redshift bin and cap and gather the files. First pre-recon\n",
    "for t in tracers:\n",
    "    for i, zs in enumerate(tracers[t]):\n",
    "        for cap in caps:\n",
    "            \n",
    "            pre_files = basepath + f\"/xi/smu/\"\n",
    "            post_files = basepath + f\"/recon_sm{recon[t][0]}/xi/smu/\"\n",
    "\n",
    "            pre_name = f\"{t}_{cap}_{zs[0]}_{zs[1]}\"\n",
    "            post_name = f\"{t}_{recon[t][1]}_{cap}_{zs[0]}_{zs[1]}\"\n",
    "            pre_cov_name = pre_name\n",
    "            post_cov_name = f\"{t}_{recon[t][1]}_sm{recon[t][0]}_{cap}_{zs[0]}_{zs[1]}\"\n",
    "            \n",
    "            data = collect_xi_data(pre_files, post_files, pre_cov_files, post_cov_files, pre_name, post_name, pre_cov_name, post_cov_name, zs, version, recon[t][0], rpcut, imaging)\n",
    "            \n",
    "            #plot_xi(data) # Plot the data to check things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769ba73-3220-4688-a992-5080e6f3bd4f",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d04c5c-1342-4e80-b8ed-ff003a0c9017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The catalogue version\n",
    "version = 1\n",
    "rpcut = False             # Whether or not to include the rpcut\n",
    "imaging = \"default_FKP\"   # What form of imaging systematics to use. Can be \"default_FKP\", \"default_FKP_addSN\", or \"default_FKP_addRF\"\n",
    "\n",
    "# This is a dictionary of all the combinations of dataset that we have and their redshift bins.\n",
    "tracers = {'BGS_BRIGHT-21.5': [[0.1, 0.4]], \n",
    "           #'LRG': [[0.4, 0.6], [0.6, 0.8], [0.8, 1.1]], \n",
    "           #'ELG_LOPnotqso': [[0.8, 1.1], [1.1, 1.6]],\n",
    "           #'QSO': [[0.8, 2.1]]\n",
    "            }\n",
    "\n",
    "# This dictionary specifies the particulars of how reconstruction was run on each tracer. First entry is smoothing scale, second is type of recon.\n",
    "recon = {'BGS_BRIGHT-21.5': [15, \"IFTrecsym\"], \n",
    "         'LRG': [10, \"IFTrecsym\"], \n",
    "         'ELG_LOPnotqso': [10, \"IFTrecsym\"],\n",
    "         'QSO': [30, \"IFTrecsym\"]}\n",
    "\n",
    "# The different sky areas\n",
    "caps = [\"NGC\", \"SGC\", \"GCcomb\"]\n",
    "\n",
    "basepath = f\"/global/cfs/cdirs/desi/survey/catalogs/Y1/LSS/iron/LSScats/v{version}/blinded/desipipe/baseline_2pt\"\n",
    "basepath_mock = f\"/global/cfs/cdirs/desi/survey/catalogs/Y1/mocks/SecondGenMocks/EZmock/desipipe/v{version}/ffa/baseline_2pt\"\n",
    "pre_cov_files = f\"/global/cfs/cdirs/desi/users/mrash/RascalC/Y1/blinded/v0.6/\"       \n",
    "post_cov_files = f\"/global/cfs/cdirs/desi/users/mrash/RascalC/Y1/blinded/v0.6/\"\n",
    "\n",
    "# Now loop over each tracer, redshift bin and cap and gather the files. First pre-recon\n",
    "for t in tracers:\n",
    "    for i, zs in enumerate(tracers[t]):\n",
    "        print(t, zs)\n",
    "        for cap in caps:\n",
    "            \n",
    "            pre_files = basepath + f\"/xi/smu/\"\n",
    "            post_files = basepath + f\"/recon_recsym/xi/smu/\"\n",
    "            pre_mock_files = None if t == \"BGS_BRIGHT-21.5\" else basepath_mock + f\"/xi/smu/\"\n",
    "            post_mock_files = None if t == \"BGS_BRIGHT-21.5\" else basepath_mock + f\"/recon_recsym/xi/smu/\"\n",
    "\n",
    "            pre_name = f\"{t}_{cap}_z{zs[0]}-{zs[1]}\"\n",
    "            post_name = f\"{t}_{cap}_z{zs[0]}-{zs[1]}\"\n",
    "            pre_cov_name = f\"{t}_{cap}_{zs[0]}_{zs[1]}\"\n",
    "            post_cov_name = f\"{t}_{recon[t][1]}_sm{recon[t][0]}_{cap}_{zs[0]}_{zs[1]}\"\n",
    "            \n",
    "            data = collect_xi_data(pre_files, post_files, pre_mock_files, post_mock_files, pre_cov_files, post_cov_files, pre_name, post_name, pre_cov_name, post_cov_name, zs, version, recon[t][0], rpcut, imaging)\n",
    "            \n",
    "            plot_xi(data) # Plot the data to check things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4856a1-d653-48bc-8222-abe704fb5fbf",
   "metadata": {},
   "source": [
    "# Test we can load the datasets in Barry\n",
    "This loads in the pickle file using the Barry routines, and then plots the data and a default model. Because I've set marg=\"full\", it will automatically find the best-fit polynomial terms too when plotting, but not the BAO parameters. If this works, a full MCMC BAO fit should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f8111-7241-4617-8c18-81c5cff12091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../../Barry/\")     # Change this so that it points to where you have Barry installed\n",
    "from barry.models import PowerBeutler2017, CorrBeutler2017\n",
    "from barry.datasets.dataset_power_spectrum import PowerSpectrum_DESI_KP4\n",
    "from barry.datasets.dataset_correlation_function import CorrelationFunction_DESI_KP4\n",
    "\n",
    "for t in tracers:\n",
    "    for i, zs in enumerate(tracers[t]):\n",
    "        for cap in caps:\n",
    "            for r in [None, \"sym\"]:\n",
    "            \n",
    "                rp = f\"{imaging}_rpcut2.5\" if rpcut else f\"{imaging}\"\n",
    "                name = f\"DESI_Y1_BLIND_v{version}_sm{recon[t][0]}_{t.lower()}_{cap.lower()}_{zs[0]}_{zs[1]}_{rp}_xi.pkl\"\n",
    "                dataset = CorrelationFunction_DESI_KP4(\n",
    "                    recon=r,\n",
    "                    fit_poles=[0, 2],\n",
    "                    min_dist=52.0,\n",
    "                    max_dist=150.0,\n",
    "                    realisation='data',\n",
    "                    num_mocks=1000,\n",
    "                    reduce_cov_factor=1,\n",
    "                    datafile=name,\n",
    "                    data_location=\"./\",\n",
    "                )\n",
    "\n",
    "                model = CorrBeutler2017(\n",
    "                    recon=dataset.recon,\n",
    "                    isotropic=dataset.isotropic,\n",
    "                    marg=\"full\",\n",
    "                    poly_poles=dataset.fit_poles,\n",
    "                    n_poly=[-2,-1,0],    # Standard polynomial terms for xi\n",
    "                )\n",
    "\n",
    "                model.set_data(dataset.get_data())\n",
    "                print(f\"{name} Worked!\")\n",
    "                #model.plot(model.get_param_dict(model.get_defaults()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d41b5-48fd-4a87-9ddb-178fbed5eae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi-main",
   "language": "python",
   "name": "cosmodesi-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
